import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import time

EXPERIMENT_NAME = "v6_recursive_bge-large-zh-v1.5"
MODEL_NAME = 'BAAI/bge-large-zh-v1.5'
CHUNK_STRATEGY = 'recursive' 
CHUNK_SIZE = 512
CHUNK_OVERLAP = 40

def load_cleaned_data(filepath):
    print(f"æ­£åœ¨åŠ è½½æ¸…æ´—åçš„æ•°æ®ä»: {filepath}")
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½æˆ–è§£ææ–‡ä»¶ {filepath}: {e}")
        return None

def chunk_documents(data, strategy, chunk_size, chunk_overlap):
    print(f"\næ­£åœ¨ä½¿ç”¨ '{strategy}' ç­–ç•¥è¿›è¡Œæ–‡æœ¬åˆ‡åˆ†...")
    
    all_chunks = []
    all_metadata = []

    for entry in data:
        full_content_parts = [entry['name']]
        if isinstance(entry.get('sections'), dict):
            for title, content in entry['sections'].items():
                text = f"{title}: {' '.join(content) if isinstance(content, list) else content}"
                full_content_parts.append(text)
        full_document_text = "\n\n".join(full_content_parts)

        base_metadata = {'name': entry['name'], 'id': entry['id'], 'url': entry['url'], 'type': entry['type']}

        if strategy == 'by_section':
            if isinstance(entry.get('sections'), dict) and entry['sections']:
                for title, content in entry['sections'].items():
                    chunk_text = f"{entry['name']} - {title}: {' '.join(content) if isinstance(content, list) else content}"
                    all_chunks.append(chunk_text)
                    all_metadata.append(base_metadata)
            else:
                all_chunks.append(full_document_text)
                all_metadata.append(base_metadata)

        elif strategy == 'recursive':
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=["\n\n", "\n", "ã€‚", "ï¼Œ", ""]
            )
            chunks = text_splitter.split_text(full_document_text)
            for chunk in chunks:
                all_chunks.append(chunk)
                all_metadata.append(base_metadata)

    print(f"åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ (Chunks)ã€‚")
    return all_chunks, all_metadata

def main():
    
    # [*** æ ¸å¿ƒä¿®æ­£ ***]
    # æ ¹æ®æ‚¨æ–°çš„æ–‡ä»¶ç»“æ„ï¼Œæ›´æ–°è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
    # '..' ä»£è¡¨ä¸Šä¸€çº§ç›®å½•ï¼Œæ‰€ä»¥ '../..' ä»£è¡¨ä¸Šä¸¤çº§ç›®å½•
    cleaned_file_path = '../cleaned_data/cleaned_medical_data_v2.json'
    model_cache_path = '../models' # å»ºè®®å°†modelsæ–‡ä»¶å¤¹æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•
    
    # è¾“å‡ºæ–‡ä»¶å¤¹å°†åˆ›å»ºåœ¨å½“å‰è„šæœ¬æ‰€åœ¨çš„ muti_model/ ç›®å½•ä¸‹
    output_directory = f'./faiss_index_{EXPERIMENT_NAME}'
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)
    
    index_filepath = os.path.join(output_directory, 'faiss.index')
    metadata_filepath = os.path.join(output_directory, 'metadata.json')
    config_filepath = os.path.join(output_directory, 'config.json')

    medical_data = load_cleaned_data(cleaned_file_path)
    if not medical_data: return

    corpus_chunks, metadata = chunk_documents(medical_data, CHUNK_STRATEGY, CHUNK_SIZE, CHUNK_OVERLAP)

    print(f"\næ­£åœ¨åŠ è½½/ä¸‹è½½ Sentence Transformer æ¨¡å‹: '{MODEL_NAME}'")
    print(f"æ‰€æœ‰æ¨¡å‹å°†è¢«ç¼“å­˜åˆ°: {os.path.abspath(model_cache_path)}")
    model = SentenceTransformer(MODEL_NAME, cache_folder=model_cache_path, device='cpu')

    print("æ¨¡å‹åŠ è½½å®Œæˆã€‚å¼€å§‹å°†æ‰€æœ‰çŸ¥è¯†ç‰‡æ®µå‘é‡åŒ–...")
    start_time = time.time()
    embeddings = model.encode(corpus_chunks, show_progress_bar=True)
    end_time = time.time()
    print(f"å‘é‡åŒ–å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’ã€‚")

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings.astype('float32'))
    print(f"\nFAISSç´¢å¼•å·²åˆ›å»ºï¼Œå…±åŒ…å« {index.ntotal} ä¸ªå‘é‡ã€‚")

    print(f"æ­£åœ¨å°†FAISSç´¢å¼•ä¿å­˜åˆ°: {index_filepath}")
    faiss.write_index(index, index_filepath)
    
    for i in range(len(metadata)):
        metadata[i]['chunk_text'] = corpus_chunks[i]

    print(f"æ­£åœ¨å°†å…ƒæ•°æ®ä¿å­˜åˆ°: {metadata_filepath}")
    with open(metadata_filepath, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    config_data = {'experiment_name': EXPERIMENT_NAME, 'model_name': MODEL_NAME}
    print(f"æ­£åœ¨å°†é…ç½®ä¿¡æ¯ä¿å­˜åˆ°: {config_filepath}")
    with open(config_filepath, 'w', encoding='utf-8') as f:
        json.dump(config_data, f, indent=2)

    print(f"\nğŸ‰ å®éªŒç‰ˆæœ¬ '{EXPERIMENT_NAME}' åˆ›å»ºæˆåŠŸï¼")

if __name__ == "__main__":
    main()
