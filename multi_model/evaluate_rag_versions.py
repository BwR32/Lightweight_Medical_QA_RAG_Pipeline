import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import os
import time

EXPERIMENTS_TO_RUN = [
    "v1_section_text2vec_base",
    "v2_recursive_text2vec_base",
    "v3_recursive_m3e_base",
    "v4_recursive_bge-large-zh-v1.5",
    "v5_recursive_bge-large-zh-v1.5",
    "v6_recursive_bge-large-zh-v1.5",
    #v1_section_text2vec_base : shibing624--text2vec-base-chinese--by_section CHUNK_SIZE = 250
    #v2_recursive_text2vec_base : shibing624--text2vec-base-chinese--recuraive CHUNK_SIZE = 250
    #v3_recursive_m3e_base : moka-ai/m3e-base -- recursive CHUNK_SIZE = 250
    #v4_recursive_bge-large-zh-v1.5 : BAAI/bge-large-zh-v1.5 --recursive CHUNK_SIZE = 250
    #v5_recursive_bge-large-zh-v1.5 : BAAI/bge-large-zh-v1.5 --recursive CHUNK_SIZE = 128
    #v6_recursive_bge-large-zh-v1.5 : BAAI/bge-large-zh-v1.5 --recursive CHUNK_SIZE = 512
]

TEST_BENCHMARK = {
    # ç—‡çŠ¶æŸ¥è¯¢
    "ä»€ä¹ˆæ˜¯åå¤´ç—›ï¼Œå®ƒå’Œæ™®é€šå¤´ç—›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ": ["åå¤´ç—›", "å¤´ç—›"],
    "ç»å¸¸æ„Ÿè§‰ç–²åŠ³ä¹åŠ›æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ": ["ç–²åŠ³", "ä¹åŠ›"],
    "æ™šä¸Šç¡è§‰æ—¶å°è…¿æŠ½ç­‹æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ": ["æŠ½ç­‹", "è‚Œè‚‰ç—‰æŒ›"],
    "çš®è‚¤ä¸Šå‡ºç°çº¢ç–¹è¿˜å¾ˆç—’ï¼Œå¯èƒ½æ˜¯å“ªäº›æƒ…å†µï¼Ÿ": ["çº¢ç–¹", "çš®ç–¹", "ç˜™ç—’"],
    "é™¤äº†æ„Ÿå†’ï¼Œè¿˜æœ‰ä»€ä¹ˆç—…ä¼šå¼•èµ·å–‰å’™ç—›ï¼Ÿ": ["å–‰å’™ç—›", "å’½ç—›"],
    "å¦‚ä½•ç¼“è§£èƒƒé…¸åæµï¼ˆçƒ§å¿ƒï¼‰çš„ç—‡çŠ¶ï¼Ÿ": ["èƒƒé…¸åæµ", "çƒ§å¿ƒ"],
    # ç–¾ç—…æŸ¥è¯¢
    "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹2å‹ç³–å°¿ç—…ã€‚": ["2å‹ç³–å°¿ç—…", "ç³–å°¿ç—…"],
    "é«˜è¡€å‹çš„è¯Šæ–­æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ": ["é«˜è¡€å‹"],
    "å¦‚ä½•é¢„é˜²éª¨è´¨ç–æ¾ï¼Ÿ": ["éª¨è´¨ç–æ¾"],
    "èƒ†ç»“çŸ³å¿…é¡»è¦åšæ‰‹æœ¯å—ï¼Ÿæœ‰å“ªäº›æ²»ç–—æ–¹æ³•ï¼Ÿ": ["èƒ†ç»“çŸ³"],
    "æŠ‘éƒç—‡çš„æ—©æœŸç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ": ["æŠ‘éƒç—‡"],
    "è¨éº»ç–¹çš„ç—…å› æ˜¯ä»€ä¹ˆï¼Ÿèƒ½æ ¹æ²»å—ï¼Ÿ": ["è¨éº»ç–¹"],
    # è¯å“æŸ¥è¯¢
    "å¸ƒæ´›èŠ¬ç¼“é‡Šèƒ¶å›Šæ˜¯ç”¨æ¥åšä»€ä¹ˆçš„ï¼Ÿ": ["å¸ƒæ´›èŠ¬"],
    "é˜¿è«è¥¿æ—çš„ç”¨æ³•ç”¨é‡å’Œæ³¨æ„äº‹é¡¹æ˜¯ä»€ä¹ˆï¼Ÿ": ["é˜¿è«è¥¿æ—"],
    "è’™è„±çŸ³æ•£æœ‰å“ªäº›å‰¯ä½œç”¨ï¼Ÿ": ["è’™è„±çŸ³æ•£"],
    "è¯·é—®â€œå·èŠå£æœæ¶²â€çš„æˆåˆ†æ˜¯ä»€ä¹ˆï¼Ÿ": ["å·èŠå£æœæ¶²"],
    "å“ºä¹³æœŸå¦‡å¥³å¯ä»¥æœç”¨å¯¹ä¹™é…°æ°¨åŸºé…šå—ï¼Ÿ": ["å¯¹ä¹™é…°æ°¨åŸºé…š"],
    "é™å‹è¯éœ€è¦ç»ˆèº«æœç”¨å—ï¼Ÿ": ["é™å‹è¯", "é«˜è¡€å‹"],
    # æ£€æŸ¥ä¸æ²»ç–—æŸ¥è¯¢
    "æ ¸ç£å…±æŒ¯ï¼ˆMRIï¼‰æ£€æŸ¥æ˜¯ç”¨æ¥åšä»€ä¹ˆçš„ï¼Ÿ": ["æ ¸ç£å…±æŒ¯"],
    "ä»€ä¹ˆæ˜¯é¶å‘æ²»ç–—ï¼Ÿ": ["é¶å‘æ²»ç–—"],
    "åšèƒƒé•œä¹‹å‰éœ€è¦åšå“ªäº›å‡†å¤‡ï¼Ÿ": ["èƒƒé•œ"],
    "â€œè¡€å¸¸è§„â€æ£€æŸ¥èƒ½æŸ¥å‡ºä»€ä¹ˆé—®é¢˜ï¼Ÿ": ["è¡€å¸¸è§„"],
    # å¤åˆä¸åœºæ™¯åŒ–æŸ¥è¯¢
    "æˆ‘æœ€è¿‘æ€»æ˜¯å¤´æ™•ï¼Œè¿˜æ¶å¿ƒæƒ³åï¼Œä¼šæ˜¯ä»€ä¹ˆç—…ï¼Ÿ": ["å¤´æ™•", "æ¶å¿ƒ", "å‘•å"],
    "å­©å­å‘çƒ§å’³å—½ï¼Œå¯ä»¥åƒç‚¹å¤´å­¢å—ï¼Ÿ": ["å‘çƒ§", "å’³å—½", "å¤´å­¢"],
    "è¿åŠ¨åè†ç›–ç–¼ï¼Œæ˜¯åº”è¯¥å†·æ•·è¿˜æ˜¯çƒ­æ•·ï¼Ÿ": ["è†ç›–ç–¼", "å…³èŠ‚ç—›"],
    "æˆ‘å¦ˆå¦ˆæœ‰é«˜è¡€å‹ï¼Œé¥®é£Ÿä¸Šåº”è¯¥æ³¨æ„äº›ä»€ä¹ˆï¼Ÿ": ["é«˜è¡€å‹"],
    "æ„Ÿå†’å’Œæµæ„Ÿæœ‰ä»€ä¹ˆä¸ä¸€æ ·ï¼Œæ€ä¹ˆåŒºåˆ†ï¼Ÿ": ["æ„Ÿå†’", "æµæ„Ÿ"],
    "é•¿æœŸå¤±çœ åº”è¯¥æ€ä¹ˆåŠï¼Œæœ‰ä»€ä¹ˆæ¨èçš„æ²»ç–—æ–¹æ³•ï¼Ÿ": ["å¤±çœ "],
    "åƒå®Œæµ·é²œåèº«ä¸Šèµ·äº†å¾ˆå¤šçº¢ç‚¹ï¼Œéå¸¸ç—’ï¼Œæˆ‘è¯¥æ€ä¹ˆåŠï¼Ÿ": ["è¿‡æ•", "è¨éº»ç–¹", "çº¢ç‚¹"],
    "ä½“æ£€å‘ç°å°¿é…¸é«˜ï¼Œéœ€è¦åƒè¯å—ï¼Ÿå¹³æ—¶è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ": ["å°¿é…¸é«˜", "ç—›é£"],
}

TOP_K = 3


class Evaluator:
    def __init__(self):
        self.models = {}
        self.indexes = {}

    def _load_model(self, model_cache_path, model_name):
        """åŠ è½½æˆ–ä»ç¼“å­˜ä¸­è·å–æ¨¡å‹"""
        if model_name in self.models:
            return self.models[model_name]
        
        print(f"\né¦–æ¬¡åŠ è½½æ¨¡å‹: '{model_name}'...")
        
        model = SentenceTransformer(model_name, cache_folder=model_cache_path, device='cpu')
        
        self.models[model_name] = model
        print("æ¨¡å‹åŠ è½½å®Œæˆã€‚")
        return model

    def _load_index(self, experiment_name):
        """åŠ è½½æˆ–ä»ç¼“å­˜ä¸­è·å–ç´¢å¼•ã€é…ç½®å’Œå…ƒæ•°æ®"""
        if experiment_name in self.indexes:
            return self.indexes[experiment_name]

        print(f"\né¦–æ¬¡åŠ è½½å®éªŒç‰ˆæœ¬ '{experiment_name}' çš„ç´¢å¼•...")
        index_directory = f'./faiss_index_{experiment_name}'
        index_filepath = os.path.join(index_directory, 'faiss.index')
        metadata_filepath = os.path.join(index_directory, 'metadata.json')
        config_filepath = os.path.join(index_directory, 'config.json')

        if not all([os.path.exists(f) for f in [index_filepath, metadata_filepath, config_filepath]]):
            print(f"é”™è¯¯: æ— æ³•åŠ è½½ '{experiment_name}'ï¼Œæ–‡ä»¶ç¼ºå¤±ã€‚è¯·æ£€æŸ¥è·¯å¾„: {index_directory}")
            return None, None, None

        index = faiss.read_index(index_filepath)
        with open(metadata_filepath, 'r', encoding='utf-8') as f: metadata = json.load(f)
        with open(config_filepath, 'r', encoding='utf-8') as f: config = json.load(f)
        
        self.indexes[experiment_name] = (index, config, metadata)
        print("ç´¢å¼•åŠ è½½æˆåŠŸã€‚")
        return index, config, metadata

    def evaluate_question(self, model_cache_path, experiment_name, question, target_keywords):
        """å¯¹å•ä¸ªé—®é¢˜è¿›è¡Œè¯„æµ‹"""
        index, config, metadata = self._load_index(experiment_name)
        if index is None:
            return False, ["åŠ è½½å¤±è´¥"]

        model_name = config.get('model_name')
        model = self._load_model(model_cache_path, model_name)

        query_vector = model.encode([question])
        _, indices = index.search(query_vector.astype('float32'), TOP_K)

        is_hit, retrieved_results = False, []
        for i in range(TOP_K):
            idx = indices[0][i]
            retrieved_chunk = metadata[idx]
            retrieved_title = retrieved_chunk.get('name', '')
            retrieved_results.append(retrieved_title)

            if any(keyword in retrieved_title for keyword in target_keywords):
                is_hit = True
        
        return is_hit, retrieved_results

def generate_report(results):
    """æ ¹æ®è¯„æµ‹ç»“æœç”ŸæˆMarkdownæŠ¥å‘Š"""
    report_content = "# RAGçŸ¥è¯†åº“ç‰ˆæœ¬è¯„æµ‹æŠ¥å‘Š\n\n## 1. æ€»ä½“æ€§èƒ½æ¦‚è§ˆ (Hit Rate @{})\n\n".format(TOP_K)
    report_content += "| å®éªŒç‰ˆæœ¬ (Experiment) | å‘½ä¸­ç‡ (Hit Rate) | å‘½ä¸­æ•°/æ€»æ•° |\n| :--- | :--- | :--- |\n"
    sorted_results = sorted(results.items(), key=lambda item: item[1]['score'], reverse=True)
    for name, res in sorted_results:
        total, hits, score_percent = res['total_questions'], res['hits'], res['score'] * 100
        bar = "â–ˆ" * int(score_percent / 5)
        report_content += f"| **{name}** | **{score_percent:.2f}%** `{bar}` | {hits} / {total} |\n"
    report_content += "\n## 2. å„é—®é¢˜è¯¦ç»†è¯„æµ‹ç»“æœ\n\n"
    report_content += "| é—®é¢˜ (Query) | " + " | ".join(results.keys()) + " |\n| :--- | " + " | ".join([":---:"] * len(results)) + " |\n"
    for i, (question, _) in enumerate(TEST_BENCHMARK.items()):
        row = f"| **Q{i+1}:** {question[:20]}... |"
        for name in results.keys():
            detail = results[name]['details'][i]
            symbol = "âœ…" if detail['hit'] else "âŒ"
            retrieved_str = ", ".join(detail['retrieved']).replace("\n", " ")
            row += f" {symbol}<br><small>*{retrieved_str}*</small> |"
        report_content += row + "\n"
    best_version = sorted_results[0][0]
    report_content += f"\n## 3. ç»“è®º\n\næ ¹æ®æœ¬æ¬¡åŸºäº {len(TEST_BENCHMARK)} ä¸ªé—®é¢˜çš„è‡ªåŠ¨åŒ–è¯„æµ‹ï¼Œ**'{best_version}'** ç‰ˆæœ¬åœ¨ Hit Rate @{TOP_K} æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ã€‚\n"
    with open("evaluation_report.md", "w", encoding="utf-8") as f: f.write(report_content)
    print("\nğŸ‰ è¯„æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: evaluation_report.md")


def main():

    evaluator = Evaluator()
    model_cache_path = '../models' 
    
    results = {}

    for experiment_name in EXPERIMENTS_TO_RUN:
        print(f"\n{'='*20} æ­£åœ¨è¯„æµ‹ç‰ˆæœ¬: {experiment_name} {'='*20}")
        hits, details = 0, []
        
        for i, (question, target_keywords) in enumerate(TEST_BENCHMARK.items()):
            print(f"  -> Q{i+1}/{len(TEST_BENCHMARK)}: {question}")
            is_hit, retrieved = evaluator.evaluate_question(model_cache_path, experiment_name, question, target_keywords)
            if is_hit: hits += 1
            details.append({'question': question, 'hit': is_hit, 'retrieved': retrieved})
            
        total = len(TEST_BENCHMARK)
        score = hits / total if total > 0 else 0
        results[experiment_name] = {'score': score, 'hits': hits, 'total_questions': total, 'details': details}
        print(f"\nç‰ˆæœ¬ '{experiment_name}' è¯„æµ‹å®Œæˆ: å‘½ä¸­ç‡ = {score*100:.2f}% ({hits}/{total})")

    generate_report(results)

if __name__ == "__main__":
    main()
